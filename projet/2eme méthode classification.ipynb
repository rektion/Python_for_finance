{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.layers as lyrs\n",
    "import tensorflow.keras.models as mod\n",
    "import sklearn.preprocessing as prepro\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nasdaq():\n",
    "    f = open('nasdaq100.csv', 'rb').readlines()[1:]\n",
    "    raw_max = []\n",
    "    raw_min = []\n",
    "    raw_dates = []\n",
    "    for b_line in f:\n",
    "        line = b_line.decode(\"utf-8\")\n",
    "        min_price = float(line.split(',')[3])\n",
    "        max_price = float(line.split(',')[2])\n",
    "        raw_min.append(min_price)\n",
    "        raw_max.append(max_price)\n",
    "    return prepro.scale(raw_max), prepro.scale(raw_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On importe le Dataset\n",
    "raw_max, raw_min = load_nasdaq()\n",
    "length = len(raw_max)\n",
    "TRAIN_SIZE = int(length*0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On déclare les tableaux qui vont stoquer les valeurs du dataset\n",
    "train_min = []\n",
    "test_min = []\n",
    "train_target_min = []\n",
    "test_target_min = []\n",
    "\n",
    "train_max = []\n",
    "test_max = []\n",
    "train_target_max = []\n",
    "test_target_max = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On stoque les données de train et de target\n",
    "# Train = Stock market des 30 derniers jours\n",
    "# Target = Stock market du lendemain\n",
    "\n",
    "for i in range(31, length):\n",
    "    if i < TRAIN_SIZE:\n",
    "        train_min.append(raw_min[i-31:i-1])\n",
    "        train_max.append(raw_max[i-31:i-1])\n",
    "        train_target_min.append(1 if raw_min[i] > raw_min[i-1] else 0)\n",
    "        train_target_max.append(1 if raw_max[i] > raw_max[i-1] else 0)\n",
    "    else:\n",
    "        test_min.append(raw_min[i-31:i-1])\n",
    "        test_max.append(raw_max[i-31:i-1])\n",
    "        test_target_min.append(1 if raw_min[i] > raw_min[i-1] else 0)\n",
    "        test_target_max.append(1 if raw_max[i] > raw_max[i-1] else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion en numpy pour des manipulations futures\n",
    "\n",
    "train_min = numpy.array(train_min)\n",
    "train_target_min = numpy.array(train_target_min)\n",
    "test_min = numpy.array(test_min)\n",
    "test_target_min = numpy.array(test_target_min)\n",
    "\n",
    "train_max = numpy.array(train_max)\n",
    "train_target_max = numpy.array(train_target_max)\n",
    "test_max = numpy.array(test_max)\n",
    "test_target_max = numpy.array(test_target_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modèle de classification du min\n",
    "model_min = mod.Sequential()\n",
    "model_min.add(lyrs.Dense(500, input_shape = (30,)))\n",
    "model_min.add(lyrs.Activation('relu'))\n",
    "model_min.add(lyrs.Dropout(0.25))\n",
    "model_min.add(lyrs.Dense(250))\n",
    "model_min.add(lyrs.Activation('relu'))\n",
    "model_min.add(lyrs.Dense(1))\n",
    "model_min.add(lyrs.Activation('softmax'))\n",
    "model_min.compile(optimizer=\"adam\", loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modèle de classification du max\n",
    "model_max = mod.Sequential()\n",
    "model_max.add(lyrs.Dense(500, input_shape = (30,)))\n",
    "model_max.add(lyrs.Activation('relu'))\n",
    "model_max.add(lyrs.Dropout(0.25))\n",
    "model_max.add(lyrs.Dense(250))\n",
    "model_max.add(lyrs.Activation('relu'))\n",
    "model_max.add(lyrs.Dense(1))\n",
    "model_max.add(lyrs.Activation('softmax'))\n",
    "model_max.compile(optimizer=\"adam\", loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 6.4372 - accuracy: 0.5779 - val_loss: 5.9908 - val_accuracy: 0.6071\n",
      "Epoch 2/6\n",
      "98/98 [==============================] - 0s 1ms/step - loss: 6.4372 - accuracy: 0.5779 - val_loss: 5.9908 - val_accuracy: 0.6071\n",
      "Epoch 3/6\n",
      "98/98 [==============================] - 0s 1ms/step - loss: 6.4372 - accuracy: 0.5779 - val_loss: 5.9908 - val_accuracy: 0.6071\n",
      "Epoch 4/6\n",
      "98/98 [==============================] - 0s 1ms/step - loss: 6.4372 - accuracy: 0.5779 - val_loss: 5.9908 - val_accuracy: 0.6071\n",
      "Epoch 5/6\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 6.4372 - accuracy: 0.5779 - val_loss: 5.9908 - val_accuracy: 0.6071\n",
      "Epoch 6/6\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 6.4372 - accuracy: 0.5779 - val_loss: 5.9908 - val_accuracy: 0.6071\n",
      " 1/98 [..............................] - ETA: 0s - loss: 7.6246 - accuracy: 0.5000WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_test_batch_end` time: 0.0040s). Check your callbacks.\n",
      "98/98 [==============================] - 0s 694us/step - loss: 6.4372 - accuracy: 0.5779\n",
      "[6.437180042266846, 0.5778688788414001]\n"
     ]
    }
   ],
   "source": [
    "history_min = model_min.fit(train_min, \n",
    "          train_target_min, \n",
    "          epochs=6,\n",
    "          batch_size = 10, \n",
    "          verbose=1, \n",
    "          validation_data=(test_min, test_target_min))\n",
    "score_min = model_min.evaluate(train_min, train_target_min, batch_size=10)\n",
    "print(score_min)\n",
    "predicted_min = model_min.predict(test_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 6.6090 - accuracy: 0.5666 - val_loss: 5.8697 - val_accuracy: 0.6151\n",
      "Epoch 2/6\n",
      "98/98 [==============================] - 0s 1ms/step - loss: 6.6090 - accuracy: 0.5666 - val_loss: 5.8697 - val_accuracy: 0.6151\n",
      "Epoch 3/6\n",
      "98/98 [==============================] - 0s 1ms/step - loss: 6.6090 - accuracy: 0.5666 - val_loss: 5.8697 - val_accuracy: 0.6151\n",
      "Epoch 4/6\n",
      "98/98 [==============================] - 0s 1ms/step - loss: 6.6090 - accuracy: 0.5666 - val_loss: 5.8697 - val_accuracy: 0.6151\n",
      "Epoch 5/6\n",
      "98/98 [==============================] - 0s 1ms/step - loss: 6.6090 - accuracy: 0.5666 - val_loss: 5.8697 - val_accuracy: 0.6151\n",
      "Epoch 6/6\n",
      "98/98 [==============================] - 0s 1ms/step - loss: 6.6090 - accuracy: 0.5666 - val_loss: 5.8697 - val_accuracy: 0.6151\n",
      "98/98 [==============================] - 0s 658us/step - loss: 6.6090 - accuracy: 0.5666\n",
      "[6.60904598236084, 0.5665983557701111]\n"
     ]
    }
   ],
   "source": [
    "history_max = model_min.fit(train_max, \n",
    "          train_target_max, \n",
    "          epochs=6,\n",
    "          batch_size = 10, \n",
    "          verbose=1, \n",
    "          validation_data=(test_max, test_target_max))\n",
    "score_max = model_min.evaluate(train_max, train_target_max, batch_size=10)\n",
    "print(score_max)\n",
    "predicted_max = model_min.predict(test_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "print(predicted_max)\n",
    "print(predicted_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le soucis avec ce réseau est qu'en prédisant sur une période aléatoire que le marché va monter \n",
    "# - Sans prendre en considération le marché des autres jours -\n",
    "# On aurait raison 61% du temps car le marché est à la hausse\n",
    "# Pour la troisième approche, nous allons tenter de l'apprentissage non-supervisé\n",
    "# Sur un marché plus constant"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
